#!/usr/bin/env python3
"""
Agentic LangChain agent with MCP tools integration.
Uses an LLM to interpret prompts and decide when to use tools.
"""
import sys
import asyncio
from pathlib import Path
from typing import List, Optional

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from langchain.agents import create_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnablePassthrough

# Import LangChain tools
from agent.langchain_tools import get_langchain_tools

# For local LLM (llama.cpp)
# Fix library path conflict with Ollama's CUDA libraries
# IMPORTANT: This must happen before any llama-cpp imports
import os
# Save original LD_LIBRARY_PATH for potential restoration
_original_ld_path = os.environ.get('LD_LIBRARY_PATH', '')
os.environ['_ORIGINAL_LD_PATH'] = _original_ld_path  # Store for debugging

try:
    from langchain_community.llms import LlamaCpp
    from langchain_community.chat_models import ChatLlamaCpp
    LLAMA_CPP_AVAILABLE = True
except (ImportError, RuntimeError) as e:
    # Restore original LD_LIBRARY_PATH if import fails
    if _original_ld_path:
        os.environ['LD_LIBRARY_PATH'] = _original_ld_path
    LLAMA_CPP_AVAILABLE = False
    error_msg = str(e)
    if isinstance(e, RuntimeError) and ('Failed to load shared library' in error_msg or 'undefined symbol' in error_msg):
        import warnings
        warnings.warn(
            f"llama-cpp-python library conflict detected: {error_msg}\n"
            f"To fix manually, run: export LD_LIBRARY_PATH=$(echo $LD_LIBRARY_PATH | tr ':' '\\n' | grep -v -i ollama | tr '\\n' ':' | sed 's/:$//')"
        )

# For OpenAI-compatible API (if available)
try:
    from langchain_openai import ChatOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


class SnowcrashAgent:
    """Agentic LLM agent with MCP tools."""
    
    def __init__(
        self,
        llm=None,
        model_path: Optional[str] = None,
        temperature: float = 0.7,
        verbose: bool = True
    ):
        """
        Initialize the agent.
        
        Args:
            llm: Pre-initialized LLM (optional)
            model_path: Path to local GGUF model (optional)
            temperature: LLM temperature
            verbose: Whether to print agent reasoning
        """
        self.verbose = verbose
        self.tools = get_langchain_tools()
        
        # Initialize LLM
        if llm is None:
            if model_path and LLAMA_CPP_AVAILABLE:
                # Use local GGUF model
                self.llm = self._create_llama_llm(model_path, temperature)
            elif OPENAI_AVAILABLE:
                # Try OpenAI-compatible API (may require API key)
                try:
                    self.llm = ChatOpenAI(temperature=temperature)
                except:
                    # Fallback to a mock LLM for testing
                    print("[WARNING] Warning: No LLM available. Using mock agent.")
                    self.llm = None
            else:
                print("[WARNING] Warning: No LLM available. Install llama-cpp-python or langchain-openai")
                self.llm = None
        else:
            self.llm = llm
        
        # Create agent if LLM is available
        if self.llm:
            self.agent_executor = self._create_agent()
        else:
            self.agent_executor = None
    
    def _create_llama_llm(self, model_path: str, temperature: float):
        """Create a LlamaCpp LLM from GGUF file."""
        if not LLAMA_CPP_AVAILABLE:
            raise ImportError("llama-cpp-python not available. Install with: pip install llama-cpp-python")
        
        from langchain_community.llms import LlamaCpp
        
        model_file = Path(model_path)
        if not model_file.exists():
            raise FileNotFoundError(f"Model not found: {model_path}")
        
        # Check if CUDA is available for GPU offloading
        try:
            import torch
            cuda_available = torch.cuda.is_available()
            if cuda_available:
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        except ImportError:
            cuda_available = False
            gpu_name = None
            gpu_memory = None
        
        # GPU offloading: use -1 to offload all layers to GPU
        # This is required even if llama-cpp-python is built with CUDA support
        n_gpu_layers = -1 if cuda_available else 0
        
        if cuda_available:
            if self.verbose:
                print(f"[INFO] GPU acceleration enabled: offloading all layers to GPU")
                print(f"[INFO] GPU: {gpu_name}, Memory: {gpu_memory:.2f} GB")
        else:
            if self.verbose:
                print(f"[WARNING] CUDA not available - SLM will run on CPU (slower)")
        
        # Hardcode LD_LIBRARY_PATH to ONLY include llama-cpp-python's library directory
        # This prevents Ollama's libraries from being found even if they're in system paths
        # This is critical because the library loading happens at LlamaCpp instantiation time
        
        # Find llama-cpp-python installation path without importing (to avoid library conflicts)
        llama_cpp_lib_dir = None
        try:
            import site
            import sysconfig
            # Try to find llama_cpp package location
            for site_packages in site.getsitepackages() + [site.getusersitepackages()]:
                llama_cpp_path = Path(site_packages) / "llama_cpp"
                lib_dir = llama_cpp_path / "lib"
                if lib_dir.exists():
                    llama_cpp_lib_dir = str(lib_dir)
                    break
        except:
            pass
        
        # If we found it, use it; otherwise try importing (might fail but worth trying)
        if not llama_cpp_lib_dir:
            try:
                import llama_cpp
                llama_cpp_lib_dir = str(Path(llama_cpp.__file__).parent / "lib")
            except ImportError:
                pass
        
        if llama_cpp_lib_dir:
            # Set LD_LIBRARY_PATH to ONLY llama-cpp-python's lib directory
            # This ensures it finds its own libraries first, before system/Ollama paths
            os.environ['LD_LIBRARY_PATH'] = llama_cpp_lib_dir
            
            if self.verbose:
                print(f"[INFO] Set LD_LIBRARY_PATH to llama-cpp-python library directory")
                print(f"[INFO]   Library path: {llama_cpp_lib_dir}")
                if os.environ.get('DEBUG_LD_PATH'):
                    original_path = os.environ.get('_ORIGINAL_LD_PATH', '')
                    if original_path:
                        print(f"[DEBUG] Original LD_LIBRARY_PATH: {original_path}")
        else:
            # Fallback: filter out ollama paths if we can't find llama-cpp-python
            current_ld_path = os.environ.get('LD_LIBRARY_PATH', '')
            if current_ld_path:
                paths = current_ld_path.split(':')
                seen = set()
                filtered_paths = []
                for p in paths:
                    p_clean = p.strip()
                    if p_clean and 'ollama' not in p_clean.lower() and p_clean not in seen:
                        filtered_paths.append(p_clean)
                        seen.add(p_clean)
                if filtered_paths:
                    os.environ['LD_LIBRARY_PATH'] = ':'.join(filtered_paths)
                else:
                    os.environ['LD_LIBRARY_PATH'] = ''
            else:
                os.environ['LD_LIBRARY_PATH'] = ''
            if self.verbose:
                print(f"[WARNING] Could not find llama-cpp-python library directory, using filtered LD_LIBRARY_PATH")
        
        return LlamaCpp(
            model_path=str(model_path),
            temperature=temperature,
            n_ctx=2048,  # Context window
            n_batch=512,  # Batch size
            verbose=self.verbose,
            n_gpu_layers=n_gpu_layers,  # Offload all layers to GPU when CUDA is available
        )
    
    def _create_agent(self):
        """Create LangChain agent with tools."""
        # Create prompt template
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful AI assistant with access to tools for object detection in images.
When the user asks you to analyze an image, detect objects, or identify what's in a picture, 
use the yolo_object_detection tool.

Be concise and helpful. Always explain what you found in the image.
If you cannot detect objects or there's an error, explain what went wrong."""),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])
        
        # Use create_agent from langchain.agents (newer API)
        try:
            # Try new API
            agent = create_agent(
                self.llm,
                self.tools,
                prompt,
                verbose=self.verbose
            )
            return agent
        except Exception as e:
            # Fallback: Create simple tool-calling agent
            print(f"[WARNING] Warning: Using fallback agent creation: {e}")
            
            # For llama.cpp models, we'll create a simpler executor
            from langchain.agents import AgentExecutor
            from langchain.agents.output_parsers import ReActJsonSingleInputOutputParser
            from langchain_core.agents import AgentAction, AgentFinish
            
            # Simple agent that can call tools
            class SimpleToolAgent:
                def __init__(self, llm, tools):
                    self.llm = llm
                    self.tools = {tool.name: tool for tool in tools}
                
                async def ainvoke(self, inputs):
                    prompt_text = inputs["input"]
                    # For now, return a simple response
                    # In production, this would use the LLM to decide tool usage
                    return {"output": f"Received: {prompt_text}. Tools available: {list(self.tools.keys())}"}
            
            agent = SimpleToolAgent(self.llm, self.tools)
            executor = AgentExecutor(
                agent=agent,
                tools=self.tools,
                verbose=self.verbose,
                handle_parsing_errors=True,
                max_iterations=5
            )
            return executor
    
    async def run(self, prompt: str, chat_history: Optional[List] = None) -> str:
        """Run the agent with a prompt."""
        if self.agent_executor is None:
            return "Error: No LLM available. Cannot run agent."
        
        try:
            # Prepare input
            inputs = {"input": prompt}
            if chat_history:
                inputs["chat_history"] = chat_history
            
            # Run agent
            if asyncio.iscoroutinefunction(self.agent_executor.ainvoke):
                result = await self.agent_executor.ainvoke(inputs)
            else:
                result = await asyncio.to_thread(self.agent_executor.invoke, inputs)
            
            return result.get("output", str(result))
            
        except Exception as e:
            error_msg = f"Agent error: {str(e)}"
            if self.verbose:
                import traceback
                traceback.print_exc()
            return error_msg
    
    def run_sync(self, prompt: str, chat_history: Optional[List] = None) -> str:
        """Synchronous wrapper for run."""
        return asyncio.run(self.run(prompt, chat_history))

